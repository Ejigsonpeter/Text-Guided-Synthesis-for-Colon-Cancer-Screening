{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe800bb0-b464-479c-97cb-6a34ad1a80f0",
   "metadata": {},
   "source": [
    "LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1eb2992b-bd59-41cf-a83b-9cd7bf788f89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0754726a2030465a87b3f3594fdc69ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc41f384-79e4-4a35-88b6-564386b1fdc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1012fd58-b8f0-4995-81a5-8f9b8ddf41ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration already exists at /home/jupyter/.cache/huggingface/accelerate/default_config.yaml, will not override. Run `accelerate config` manually or pass a different `save_location`.\n"
     ]
    }
   ],
   "source": [
    "!accelerate config default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78171597-25b7-4462-905f-12ea7e0453fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "INSTANCE_DIR=\"colonoscopy_data_for_vqa/data\"\n",
    "OUTPUT_DIR=\"lora-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41ceed52-ab89-4674-965e-3ae20a0ea5e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "06/05/2024 08:48:34 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 3\n",
      "Local process index: 3\n",
      "Device: cuda:3\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "06/05/2024 08:48:34 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 2\n",
      "Local process index: 2\n",
      "Device: cuda:2\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "06/05/2024 08:48:34 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 1\n",
      "Local process index: 1\n",
      "Device: cuda:1\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "06/05/2024 08:48:34 - INFO - __main__ - Distributed environment: MULTI_GPU  Backend: nccl\n",
      "Num processes: 4\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'dynamic_thresholding_ratio', 'prediction_type', 'variance_type', 'timestep_spacing', 'rescale_betas_zero_snr', 'sample_max_value', 'thresholding'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "{'addition_time_embed_dim', 'only_cross_attention', 'use_linear_projection', 'encoder_hid_dim', 'encoder_hid_dim_type', 'dual_cross_attention', 'dropout', 'projection_class_embeddings_input_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act', 'resnet_out_scale_factor', 'class_embed_type', 'num_attention_heads', 'upcast_attention', 'class_embeddings_concat', 'time_embedding_act_fn', 'time_cond_proj_dim', 'cross_attention_norm', 'timestep_post_act', 'addition_embed_type', 'mid_block_only_cross_attention', 'conv_out_kernel', 'attention_type', 'resnet_time_scale_shift', 'mid_block_type', 'num_class_embeds', 'transformer_layers_per_block', 'conv_in_kernel', 'time_embedding_dim', 'time_embedding_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "06/05/2024 08:48:40 - INFO - __main__ - ***** Running training *****\n",
      "06/05/2024 08:48:40 - INFO - __main__ -   Num examples = 1890\n",
      "06/05/2024 08:48:40 - INFO - __main__ -   Num batches each epoch = 473\n",
      "06/05/2024 08:48:40 - INFO - __main__ -   Num Epochs = 1\n",
      "06/05/2024 08:48:40 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "06/05/2024 08:48:40 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "06/05/2024 08:48:40 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "06/05/2024 08:48:40 - INFO - __main__ -   Total optimization steps = 100\n",
      "Steps:   1%|            | 1/100 [00:05<09:20,  5.67s/it, loss=0.0437, lr=0.0001]06/05/2024 08:48:46 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "06/05/2024 08:48:46 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "06/05/2024 08:48:46 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "06/05/2024 08:48:46 - INFO - torch.nn.parallel.distributed - Reducer buckets have been rebuilt in this iteration.\n",
      "Steps: 100%|██████████| 100/100 [02:22<00:00,  1.39s/it, loss=0.0298, lr=0.0001]06/05/2024 08:51:03 - INFO - accelerate.accelerator - Saving current state to lora-finetuned/checkpoint-100\n",
      "Model weights saved in lora-finetuned/checkpoint-100/pytorch_lora_weights.safetensors\n",
      "06/05/2024 08:51:03 - INFO - accelerate.checkpointing - Optimizer state saved in lora-finetuned/checkpoint-100/optimizer.bin\n",
      "06/05/2024 08:51:03 - INFO - accelerate.checkpointing - Scheduler state saved in lora-finetuned/checkpoint-100/scheduler.bin\n",
      "06/05/2024 08:51:03 - INFO - accelerate.checkpointing - Sampler state for dataloader 0 saved in lora-finetuned/checkpoint-100/sampler.bin\n",
      "06/05/2024 08:51:03 - INFO - accelerate.checkpointing - Random states saved in lora-finetuned/checkpoint-100/random_states_0.pkl\n",
      "06/05/2024 08:51:03 - INFO - __main__ - Saved state to lora-finetuned/checkpoint-100\n",
      "Steps: 100%|██████████| 100/100 [02:22<00:00,  1.39s/it, loss=0.0187, lr=0.0001]\n",
      "model_index.json: 100%|████████████████████████| 541/541 [00:00<00:00, 5.05MB/s]\u001b[A\n",
      "\n",
      "Fetching 11 files:   0%|                                 | 0/11 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "safety_checker/config.json: 100%|██████████| 4.72k/4.72k [00:00<00:00, 35.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "(…)ature_extractor/preprocessor_config.json: 100%|█| 342/342 [00:00<00:00, 3.62M\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "model.safetensors:   0%|                            | 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   4%|▊                   | 52.4M/1.22G [00:00<00:02, 466MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   9%|█▊                   | 105M/1.22G [00:00<00:02, 489MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  13%|██▋                  | 157M/1.22G [00:00<00:02, 501MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|███▌                 | 210M/1.22G [00:00<00:02, 502MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  22%|████▌                | 262M/1.22G [00:00<00:01, 504MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  26%|█████▍               | 315M/1.22G [00:00<00:01, 504MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  30%|██████▎              | 367M/1.22G [00:00<00:01, 504MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|███████▏             | 419M/1.22G [00:00<00:01, 502MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  39%|████████▏            | 472M/1.22G [00:00<00:01, 504MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  43%|█████████            | 524M/1.22G [00:01<00:01, 508MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  47%|█████████▉           | 577M/1.22G [00:01<00:01, 509MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  52%|██████████▊          | 629M/1.22G [00:01<00:01, 509MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  56%|███████████▊         | 682M/1.22G [00:01<00:01, 512MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  60%|████████████▋        | 734M/1.22G [00:01<00:00, 511MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  65%|█████████████▌       | 786M/1.22G [00:01<00:00, 510MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  69%|██████████████▍      | 839M/1.22G [00:01<00:00, 510MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  73%|███████████████▍     | 891M/1.22G [00:01<00:00, 510MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  78%|████████████████▎    | 944M/1.22G [00:01<00:00, 514MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  82%|█████████████████▏   | 996M/1.22G [00:01<00:00, 514MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  86%|█████████████████▏  | 1.05G/1.22G [00:02<00:00, 513MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  91%|██████████████████  | 1.10G/1.22G [00:02<00:00, 512MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  95%|██████████████████▉ | 1.15G/1.22G [00:02<00:00, 513MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|████████████████████| 1.22G/1.22G [00:02<00:00, 506MB/s]\u001b[A\u001b[A\n",
      "\n",
      "Fetching 11 files: 100%|████████████████████████| 11/11 [00:02<00:00,  4.39it/s]\u001b[A\n",
      "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  71%|█████████▎   | 5/7 [00:00<00:00, 19.11it/s]\u001b[A{'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 23.45it/s]\n",
      "06/05/2024 08:51:06 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A photo with 1 polyp.\n",
      "{'thresholding', 'euler_at_final', 'dynamic_thresholding_ratio', 'prediction_type', 'solver_order', 'use_lu_lambdas', 'rescale_betas_zero_snr', 'variance_type', 'lower_order_final', 'final_sigmas_type', 'timestep_spacing', 'sample_max_value', 'use_karras_sigmas', 'lambda_min_clipped', 'algorithm_type', 'solver_type'} was not found in config. Values will be initialized to default values.\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "Model weights saved in lora-finetuned/pytorch_lora_weights.safetensors\n",
      "{'image_encoder', 'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:00, 13.58it/s]\u001b[A{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  71%|█████████▎   | 5/7 [00:00<00:00, 11.55it/s]\u001b[A{'addition_time_embed_dim', 'only_cross_attention', 'use_linear_projection', 'encoder_hid_dim', 'encoder_hid_dim_type', 'dual_cross_attention', 'dropout', 'projection_class_embeddings_input_dim', 'addition_embed_type_num_heads', 'resnet_skip_time_act', 'resnet_out_scale_factor', 'class_embed_type', 'num_attention_heads', 'upcast_attention', 'class_embeddings_concat', 'time_embedding_act_fn', 'time_cond_proj_dim', 'cross_attention_norm', 'timestep_post_act', 'addition_embed_type', 'mid_block_only_cross_attention', 'conv_out_kernel', 'attention_type', 'resnet_time_scale_shift', 'mid_block_type', 'num_class_embeds', 'transformer_layers_per_block', 'conv_in_kernel', 'time_embedding_dim', 'time_embedding_type', 'reverse_transformer_layers_per_block'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'latents_mean', 'force_upcast', 'latents_std', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:00<00:00, 12.13it/s]\u001b[A\n",
      "Loading unet.\n",
      "06/05/2024 08:53:38 - INFO - __main__ - Running validation... \n",
      " Generating 4 images with prompt: A photo with 1 polyp.\n",
      "{'thresholding', 'euler_at_final', 'dynamic_thresholding_ratio', 'prediction_type', 'solver_order', 'use_lu_lambdas', 'rescale_betas_zero_snr', 'variance_type', 'lower_order_final', 'final_sigmas_type', 'timestep_spacing', 'sample_max_value', 'use_karras_sigmas', 'lambda_min_clipped', 'algorithm_type', 'solver_type'} was not found in config. Values will be initialized to default values.\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "Steps: 100%|██████████| 100/100 [06:18<00:00,  3.79s/it, loss=0.0187, lr=0.0001]\n"
     ]
    }
   ],
   "source": [
    "from diffusers.models.modeling_outputs import Transformer2DModelOutput\n",
    "\n",
    "!accelerate launch diffusers/examples/dreambooth/train_dreambooth_lora.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --instance_data_dir=$INSTANCE_DIR \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --instance_prompt=\"Generate an image of a colon\" \\\n",
    "  --resolution=512 \\\n",
    "  --train_batch_size=1 \\\n",
    "  --gradient_accumulation_steps=1 \\\n",
    "  --checkpointing_steps=100 \\\n",
    "  --learning_rate=1e-4 \\\n",
    "  --lr_scheduler=\"constant\" \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=100 \\\n",
    "  --validation_prompt=\"A photo with 1 polyp\" \\\n",
    "  --validation_epochs=50 \\\n",
    "  --seed=\"0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0c07b-a84c-4f48-a3b8-be29b9ae2639",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e82463-25a5-4944-8e44-9ae76637fff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588aae0b-deae-48b4-bb82-78828565f51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a05fac19-619f-46cd-84a9-b4a699fa503f",
   "metadata": {},
   "source": [
    "## LORA inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3793a4b0-e94f-4a7a-b4df-2335b582d94b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2beface568c442d39a985b88572745ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69cdea6f9d44f379cf929e7798c7d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"lora-finetuned\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe.unet.load_attn_procs(model_path)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"Generate an image from a gastroscopy procedure\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5, cross_attention_kwargs={\"scale\": 0.5}).images[0]\n",
    "image.save(\"colon2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0b3a5cb-b057-4b84-9cbf-b6e131040688",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d368a527a17640488113881a7a9b08d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c144e90c48ab489eaf4b7aa3bf318322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_path = \"lora-finetuned\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe.unet.load_attn_procs(model_path)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "prompt = \"Medical illustration of a gastroscopy procedure\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5, cross_attention_kwargs={\"scale\": 0.5}).images[0]\n",
    "image.save(\"gastroscopy_procedure.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47788c73-ed22-4932-b4bd-fb7dcd4be1fa",
   "metadata": {},
   "source": [
    "## MERGED LORA + ORIGINAL Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c8846c9-1868-4199-84d6-13f884e80fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2c08c8fedb429e8e4bf64a6cc6ee1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a9bde3126e417f8809182a84930dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and the stable diffusion pipeline\n",
    "model_path = \"lora-finetuned\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"finetuned-stable-diffusion\", torch_dtype=torch.float16)\n",
    "pipe.unet.load_attn_procs(model_path)\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "# Disable the safety checker\n",
    "def dummy_checker(images, **kwargs):\n",
    "    # Return images and a list of False, one for each image\n",
    "    return images, [False] * len(images)\n",
    "\n",
    "pipe.safety_checker = dummy_checker\n",
    "\n",
    "# Generate the image\n",
    "prompt = \"Generate an image with an abnormality with the colors pink, red, yellow and white\"\n",
    "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5, cross_attention_kwargs={\"scale\": 0.5}).images[0]\n",
    "image.save(\"gastroscopy_procedure2.png\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
